// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"time"

	"go.opentelemetry.io/collector/component"
	"go.opentelemetry.io/collector/confmap"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
)

// MetricSettings provides common settings for a particular metric.
type MetricSettings struct {
	Enabled bool `mapstructure:"enabled"`

	enabledProvidedByUser bool
}

// IsEnabledProvidedByUser returns true if `enabled` option is explicitly set in user settings to any value.
func (ms *MetricSettings) IsEnabledProvidedByUser() bool {
	return ms.enabledProvidedByUser
}

func (ms *MetricSettings) Unmarshal(parser *confmap.Conf) error {
	if parser == nil {
		return nil
	}
	err := parser.Unmarshal(ms, confmap.WithErrorUnused())
	if err != nil {
		return err
	}
	ms.enabledProvidedByUser = parser.IsSet("enabled")
	return nil
}

// MetricsSettings provides settings for snowflakereceiver metrics.
type MetricsSettings struct {
	SnowflakeBillingCloudServiceTotal              MetricSettings `mapstructure:"snowflake.billing.cloud_service.total"`
	SnowflakeBillingTotalCreditTotal               MetricSettings `mapstructure:"snowflake.billing.total_credit.total"`
	SnowflakeBillingVirtualWarehouseTotal          MetricSettings `mapstructure:"snowflake.billing.virtual_warehouse.total"`
	SnowflakeBillingWarehouseCloudServiceTotal     MetricSettings `mapstructure:"snowflake.billing.warehouse.cloud_service.total"`
	SnowflakeBillingWarehouseTotalCreditTotal      MetricSettings `mapstructure:"snowflake.billing.warehouse.total_credit.total"`
	SnowflakeBillingWarehouseVirtualWarehouseTotal MetricSettings `mapstructure:"snowflake.billing.warehouse.virtual_warehouse.total"`
	SnowflakeDatabaseBytesScannedAvg               MetricSettings `mapstructure:"snowflake.database.bytes_scanned.avg"`
	SnowflakeDatabaseQueryCount                    MetricSettings `mapstructure:"snowflake.database.query.count"`
	SnowflakeLoginsTotal                           MetricSettings `mapstructure:"snowflake.logins.total"`
	SnowflakePipeCreditsUsedTotal                  MetricSettings `mapstructure:"snowflake.pipe.credits_used.total"`
	SnowflakeQueryBlocked                          MetricSettings `mapstructure:"snowflake.query.blocked"`
	SnowflakeQueryBytesDeletedTotal                MetricSettings `mapstructure:"snowflake.query.bytes_deleted.total"`
	SnowflakeQueryBytesScannedTotal                MetricSettings `mapstructure:"snowflake.query.bytes_scanned.total"`
	SnowflakeQueryBytesSpilledLocalTotal           MetricSettings `mapstructure:"snowflake.query.bytes_spilled.local.total"`
	SnowflakeQueryBytesSpilledRemoteTotal          MetricSettings `mapstructure:"snowflake.query.bytes_spilled.remote.total"`
	SnowflakeQueryBytesWrittenTotal                MetricSettings `mapstructure:"snowflake.query.bytes_written.total"`
	SnowflakeQueryCompilationTimeTotal             MetricSettings `mapstructure:"snowflake.query.compilation_time.total"`
	SnowflakeQueryDataScannedCacheAvg              MetricSettings `mapstructure:"snowflake.query.data_scanned_cache.avg"`
	SnowflakeQueryExecuted                         MetricSettings `mapstructure:"snowflake.query.executed"`
	SnowflakeQueryExecutionTimeTotal               MetricSettings `mapstructure:"snowflake.query.execution_time.total"`
	SnowflakeQueryPartitionsScannedTotal           MetricSettings `mapstructure:"snowflake.query.partitions_scanned.total"`
	SnowflakeQueryQueuedOverload                   MetricSettings `mapstructure:"snowflake.query.queued_overload"`
	SnowflakeQueryQueuedProvision                  MetricSettings `mapstructure:"snowflake.query.queued_provision"`
	SnowflakeQueuedOverloadTimeAvg                 MetricSettings `mapstructure:"snowflake.queued_overload_time.avg"`
	SnowflakeQueuedOverloadTimeTotal               MetricSettings `mapstructure:"snowflake.queued_overload_time.total"`
	SnowflakeQueuedProvisioningTimeAvg             MetricSettings `mapstructure:"snowflake.queued_provisioning_time.avg"`
	SnowflakeQueuedProvisioningTimeTotal           MetricSettings `mapstructure:"snowflake.queued_provisioning_time.total"`
	SnowflakeQueuedRepairTimeAvg                   MetricSettings `mapstructure:"snowflake.queued_repair_time.avg"`
	SnowflakeQueuedRepairTimeTotal                 MetricSettings `mapstructure:"snowflake.queued_repair_time.total"`
	SnowflakeRowsDeletedTotal                      MetricSettings `mapstructure:"snowflake.rows_deleted.total"`
	SnowflakeRowsInsertedTotal                     MetricSettings `mapstructure:"snowflake.rows_inserted.total"`
	SnowflakeRowsProducedTotal                     MetricSettings `mapstructure:"snowflake.rows_produced.total"`
	SnowflakeRowsUnloadedTotal                     MetricSettings `mapstructure:"snowflake.rows_unloaded.total"`
	SnowflakeRowsUpdatedTotal                      MetricSettings `mapstructure:"snowflake.rows_updated.total"`
	SnowflakeSessionIDCount                        MetricSettings `mapstructure:"snowflake.session_id.count"`
	SnowflakeStorageFailsafeBytesTotal             MetricSettings `mapstructure:"snowflake.storage.failsafe_bytes.total"`
	SnowflakeStorageStageBytesTotal                MetricSettings `mapstructure:"snowflake.storage.stage_bytes.total"`
	SnowflakeStorageStorageBytesTotal              MetricSettings `mapstructure:"snowflake.storage.storage_bytes.total"`
	SnowflakeTotalElapsedTimeAvg                   MetricSettings `mapstructure:"snowflake.total_elapsed_time.avg"`
	SnowflakeTotalElapsedTimeTotal                 MetricSettings `mapstructure:"snowflake.total_elapsed_time.total"`
}

func DefaultMetricsSettings() MetricsSettings {
	return MetricsSettings{
		SnowflakeBillingCloudServiceTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeBillingTotalCreditTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeBillingVirtualWarehouseTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeBillingWarehouseCloudServiceTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeBillingWarehouseTotalCreditTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeBillingWarehouseVirtualWarehouseTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeDatabaseBytesScannedAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeDatabaseQueryCount: MetricSettings{
			Enabled: true,
		},
		SnowflakeLoginsTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakePipeCreditsUsedTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeQueryBlocked: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryBytesDeletedTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryBytesScannedTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeQueryBytesSpilledLocalTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeQueryBytesSpilledRemoteTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeQueryBytesWrittenTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryCompilationTimeTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryDataScannedCacheAvg: MetricSettings{
			Enabled: false,
		},
		SnowflakeQueryExecuted: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryExecutionTimeTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryPartitionsScannedTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeQueryQueuedOverload: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueryQueuedProvision: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedOverloadTimeAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedOverloadTimeTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeQueuedProvisioningTimeAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedProvisioningTimeTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeQueuedRepairTimeAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeQueuedRepairTimeTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeRowsDeletedTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeRowsInsertedTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeRowsProducedTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeRowsUnloadedTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeRowsUpdatedTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeSessionIDCount: MetricSettings{
			Enabled: false,
		},
		SnowflakeStorageFailsafeBytesTotal: MetricSettings{
			Enabled: false,
		},
		SnowflakeStorageStageBytesTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeStorageStorageBytesTotal: MetricSettings{
			Enabled: true,
		},
		SnowflakeTotalElapsedTimeAvg: MetricSettings{
			Enabled: true,
		},
		SnowflakeTotalElapsedTimeTotal: MetricSettings{
			Enabled: false,
		},
	}
}

type metricSnowflakeBillingCloudServiceTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.cloud_service.total metric with initial data.
func (m *metricSnowflakeBillingCloudServiceTotal) init() {
	m.data.SetName("snowflake.billing.cloud_service.total")
	m.data.SetDescription("Reported total credits used in the cloud service.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingCloudServiceTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service_type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingCloudServiceTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingCloudServiceTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingCloudServiceTotal(settings MetricSettings) metricSnowflakeBillingCloudServiceTotal {
	m := metricSnowflakeBillingCloudServiceTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingTotalCreditTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.total_credit.total metric with initial data.
func (m *metricSnowflakeBillingTotalCreditTotal) init() {
	m.data.SetName("snowflake.billing.total_credit.total")
	m.data.SetDescription("Reported total credits used across account.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingTotalCreditTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service_type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingTotalCreditTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingTotalCreditTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingTotalCreditTotal(settings MetricSettings) metricSnowflakeBillingTotalCreditTotal {
	m := metricSnowflakeBillingTotalCreditTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingVirtualWarehouseTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.virtual_warehouse.total metric with initial data.
func (m *metricSnowflakeBillingVirtualWarehouseTotal) init() {
	m.data.SetName("snowflake.billing.virtual_warehouse.total")
	m.data.SetDescription("Reported total credits used by virtual warehouse service.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingVirtualWarehouseTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("service_type", serviceTypeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingVirtualWarehouseTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingVirtualWarehouseTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingVirtualWarehouseTotal(settings MetricSettings) metricSnowflakeBillingVirtualWarehouseTotal {
	m := metricSnowflakeBillingVirtualWarehouseTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseCloudServiceTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.cloud_service.total metric with initial data.
func (m *metricSnowflakeBillingWarehouseCloudServiceTotal) init() {
	m.data.SetName("snowflake.billing.warehouse.cloud_service.total")
	m.data.SetDescription("Credits used across cloud service for given warehouse.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseCloudServiceTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseCloudServiceTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseCloudServiceTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseCloudServiceTotal(settings MetricSettings) metricSnowflakeBillingWarehouseCloudServiceTotal {
	m := metricSnowflakeBillingWarehouseCloudServiceTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseTotalCreditTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.total_credit.total metric with initial data.
func (m *metricSnowflakeBillingWarehouseTotalCreditTotal) init() {
	m.data.SetName("snowflake.billing.warehouse.total_credit.total")
	m.data.SetDescription("Total credits used associated with given warehouse.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseTotalCreditTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseTotalCreditTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseTotalCreditTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseTotalCreditTotal(settings MetricSettings) metricSnowflakeBillingWarehouseTotalCreditTotal {
	m := metricSnowflakeBillingWarehouseTotalCreditTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeBillingWarehouseVirtualWarehouseTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.billing.warehouse.virtual_warehouse.total metric with initial data.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseTotal) init() {
	m.data.SetName("snowflake.billing.warehouse.virtual_warehouse.total")
	m.data.SetDescription("Total credits used by virtual warehouse service for given warehouse.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeBillingWarehouseVirtualWarehouseTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeBillingWarehouseVirtualWarehouseTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeBillingWarehouseVirtualWarehouseTotal(settings MetricSettings) metricSnowflakeBillingWarehouseVirtualWarehouseTotal {
	m := metricSnowflakeBillingWarehouseVirtualWarehouseTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeDatabaseBytesScannedAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.database.bytes_scanned.avg metric with initial data.
func (m *metricSnowflakeDatabaseBytesScannedAvg) init() {
	m.data.SetName("snowflake.database.bytes_scanned.avg")
	m.data.SetDescription("Average bytes scanned in a database.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeDatabaseBytesScannedAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeDatabaseBytesScannedAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeDatabaseBytesScannedAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeDatabaseBytesScannedAvg(settings MetricSettings) metricSnowflakeDatabaseBytesScannedAvg {
	m := metricSnowflakeDatabaseBytesScannedAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeDatabaseQueryCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.database.query.count metric with initial data.
func (m *metricSnowflakeDatabaseQueryCount) init() {
	m.data.SetName("snowflake.database.query.count")
	m.data.SetDescription("Total query count for database.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeDatabaseQueryCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeDatabaseQueryCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeDatabaseQueryCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeDatabaseQueryCount(settings MetricSettings) metricSnowflakeDatabaseQueryCount {
	m := metricSnowflakeDatabaseQueryCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeLoginsTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.logins.total metric with initial data.
func (m *metricSnowflakeLoginsTotal) init() {
	m.data.SetName("snowflake.logins.total")
	m.data.SetDescription("Total login attempts for account.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeLoginsTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, errorMessageAttributeValue string, reportedClientTypeAttributeValue string, isSuccessAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("reported_client_type", reportedClientTypeAttributeValue)
	dp.Attributes().PutStr("is_success", isSuccessAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeLoginsTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeLoginsTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeLoginsTotal(settings MetricSettings) metricSnowflakeLoginsTotal {
	m := metricSnowflakeLoginsTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakePipeCreditsUsedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.pipe.credits_used.total metric with initial data.
func (m *metricSnowflakePipeCreditsUsedTotal) init() {
	m.data.SetName("snowflake.pipe.credits_used.total")
	m.data.SetDescription("Snow pipe credits contotaled.")
	m.data.SetUnit("{credits}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakePipeCreditsUsedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, pipeNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("pipe_name", pipeNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakePipeCreditsUsedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakePipeCreditsUsedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakePipeCreditsUsedTotal(settings MetricSettings) metricSnowflakePipeCreditsUsedTotal {
	m := metricSnowflakePipeCreditsUsedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBlocked struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.blocked metric with initial data.
func (m *metricSnowflakeQueryBlocked) init() {
	m.data.SetName("snowflake.query.blocked")
	m.data.SetDescription("Blocked query count for warehouse.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBlocked) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBlocked) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBlocked) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBlocked(settings MetricSettings) metricSnowflakeQueryBlocked {
	m := metricSnowflakeQueryBlocked{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesDeletedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_deleted.total metric with initial data.
func (m *metricSnowflakeQueryBytesDeletedTotal) init() {
	m.data.SetName("snowflake.query.bytes_deleted.total")
	m.data.SetDescription("Total bytes deleted in database.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesDeletedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesDeletedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesDeletedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesDeletedTotal(settings MetricSettings) metricSnowflakeQueryBytesDeletedTotal {
	m := metricSnowflakeQueryBytesDeletedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesScannedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_scanned.total metric with initial data.
func (m *metricSnowflakeQueryBytesScannedTotal) init() {
	m.data.SetName("snowflake.query.bytes_scanned.total")
	m.data.SetDescription("Total bytes scanend in database.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesScannedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesScannedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesScannedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesScannedTotal(settings MetricSettings) metricSnowflakeQueryBytesScannedTotal {
	m := metricSnowflakeQueryBytesScannedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesSpilledLocalTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_spilled.local.total metric with initial data.
func (m *metricSnowflakeQueryBytesSpilledLocalTotal) init() {
	m.data.SetName("snowflake.query.bytes_spilled.local.total")
	m.data.SetDescription("Total bytes spilled (intermediate results do not fit in memory) by local storage.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesSpilledLocalTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesSpilledLocalTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesSpilledLocalTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesSpilledLocalTotal(settings MetricSettings) metricSnowflakeQueryBytesSpilledLocalTotal {
	m := metricSnowflakeQueryBytesSpilledLocalTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesSpilledRemoteTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_spilled.remote.total metric with initial data.
func (m *metricSnowflakeQueryBytesSpilledRemoteTotal) init() {
	m.data.SetName("snowflake.query.bytes_spilled.remote.total")
	m.data.SetDescription("Total bytes spilled (intermediate results do not fit in memory) by remote storage.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesSpilledRemoteTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesSpilledRemoteTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesSpilledRemoteTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesSpilledRemoteTotal(settings MetricSettings) metricSnowflakeQueryBytesSpilledRemoteTotal {
	m := metricSnowflakeQueryBytesSpilledRemoteTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryBytesWrittenTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.bytes_written.total metric with initial data.
func (m *metricSnowflakeQueryBytesWrittenTotal) init() {
	m.data.SetName("snowflake.query.bytes_written.total")
	m.data.SetDescription("Total bytes written by database.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryBytesWrittenTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryBytesWrittenTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryBytesWrittenTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryBytesWrittenTotal(settings MetricSettings) metricSnowflakeQueryBytesWrittenTotal {
	m := metricSnowflakeQueryBytesWrittenTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryCompilationTimeTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.compilation_time.total metric with initial data.
func (m *metricSnowflakeQueryCompilationTimeTotal) init() {
	m.data.SetName("snowflake.query.compilation_time.total")
	m.data.SetDescription("Total time taken to compile query.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryCompilationTimeTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryCompilationTimeTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryCompilationTimeTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryCompilationTimeTotal(settings MetricSettings) metricSnowflakeQueryCompilationTimeTotal {
	m := metricSnowflakeQueryCompilationTimeTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryDataScannedCacheAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.data_scanned_cache.avg metric with initial data.
func (m *metricSnowflakeQueryDataScannedCacheAvg) init() {
	m.data.SetName("snowflake.query.data_scanned_cache.avg")
	m.data.SetDescription("Average percentage of data scanned from cache.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryDataScannedCacheAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryDataScannedCacheAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryDataScannedCacheAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryDataScannedCacheAvg(settings MetricSettings) metricSnowflakeQueryDataScannedCacheAvg {
	m := metricSnowflakeQueryDataScannedCacheAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryExecuted struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.executed metric with initial data.
func (m *metricSnowflakeQueryExecuted) init() {
	m.data.SetName("snowflake.query.executed")
	m.data.SetDescription("Executed query count for warehouse.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryExecuted) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryExecuted) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryExecuted) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryExecuted(settings MetricSettings) metricSnowflakeQueryExecuted {
	m := metricSnowflakeQueryExecuted{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryExecutionTimeTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.execution_time.total metric with initial data.
func (m *metricSnowflakeQueryExecutionTimeTotal) init() {
	m.data.SetName("snowflake.query.execution_time.total")
	m.data.SetDescription("Total time spent executing queries in database.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryExecutionTimeTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryExecutionTimeTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryExecutionTimeTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryExecutionTimeTotal(settings MetricSettings) metricSnowflakeQueryExecutionTimeTotal {
	m := metricSnowflakeQueryExecutionTimeTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryPartitionsScannedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.partitions_scanned.total metric with initial data.
func (m *metricSnowflakeQueryPartitionsScannedTotal) init() {
	m.data.SetName("snowflake.query.partitions_scanned.total")
	m.data.SetDescription("Number of partitions scanned during query so far.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryPartitionsScannedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryPartitionsScannedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryPartitionsScannedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryPartitionsScannedTotal(settings MetricSettings) metricSnowflakeQueryPartitionsScannedTotal {
	m := metricSnowflakeQueryPartitionsScannedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryQueuedOverload struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.queued_overload metric with initial data.
func (m *metricSnowflakeQueryQueuedOverload) init() {
	m.data.SetName("snowflake.query.queued_overload")
	m.data.SetDescription("Overloaded query count for warehouse.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryQueuedOverload) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryQueuedOverload) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryQueuedOverload) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryQueuedOverload(settings MetricSettings) metricSnowflakeQueryQueuedOverload {
	m := metricSnowflakeQueryQueuedOverload{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueryQueuedProvision struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.query.queued_provision metric with initial data.
func (m *metricSnowflakeQueryQueuedProvision) init() {
	m.data.SetName("snowflake.query.queued_provision")
	m.data.SetDescription("Number of compute resources queued for provisioning.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueryQueuedProvision) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueryQueuedProvision) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueryQueuedProvision) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueryQueuedProvision(settings MetricSettings) metricSnowflakeQueryQueuedProvision {
	m := metricSnowflakeQueryQueuedProvision{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedOverloadTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_overload_time.avg metric with initial data.
func (m *metricSnowflakeQueuedOverloadTimeAvg) init() {
	m.data.SetName("snowflake.queued_overload_time.avg")
	m.data.SetDescription("Average time spent in warehouse queue due to warehouse being overloaded.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedOverloadTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedOverloadTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedOverloadTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedOverloadTimeAvg(settings MetricSettings) metricSnowflakeQueuedOverloadTimeAvg {
	m := metricSnowflakeQueuedOverloadTimeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedOverloadTimeTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_overload_time.total metric with initial data.
func (m *metricSnowflakeQueuedOverloadTimeTotal) init() {
	m.data.SetName("snowflake.queued_overload_time.total")
	m.data.SetDescription("Total time spent in warehouse queue due to warehouse being overloaded.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedOverloadTimeTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedOverloadTimeTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedOverloadTimeTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedOverloadTimeTotal(settings MetricSettings) metricSnowflakeQueuedOverloadTimeTotal {
	m := metricSnowflakeQueuedOverloadTimeTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedProvisioningTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_provisioning_time.avg metric with initial data.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) init() {
	m.data.SetName("snowflake.queued_provisioning_time.avg")
	m.data.SetDescription("Average time spent in warehouse queue waiting for resources to provision.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedProvisioningTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedProvisioningTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedProvisioningTimeAvg(settings MetricSettings) metricSnowflakeQueuedProvisioningTimeAvg {
	m := metricSnowflakeQueuedProvisioningTimeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedProvisioningTimeTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_provisioning_time.total metric with initial data.
func (m *metricSnowflakeQueuedProvisioningTimeTotal) init() {
	m.data.SetName("snowflake.queued_provisioning_time.total")
	m.data.SetDescription("Total time spent in warehouse queue waiting for resources to provision.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedProvisioningTimeTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedProvisioningTimeTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedProvisioningTimeTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedProvisioningTimeTotal(settings MetricSettings) metricSnowflakeQueuedProvisioningTimeTotal {
	m := metricSnowflakeQueuedProvisioningTimeTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedRepairTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_repair_time.avg metric with initial data.
func (m *metricSnowflakeQueuedRepairTimeAvg) init() {
	m.data.SetName("snowflake.queued_repair_time.avg")
	m.data.SetDescription("Average time spent in warehouse queue waiting for compute resources to be repaired.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedRepairTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedRepairTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedRepairTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedRepairTimeAvg(settings MetricSettings) metricSnowflakeQueuedRepairTimeAvg {
	m := metricSnowflakeQueuedRepairTimeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeQueuedRepairTimeTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.queued_repair_time.total metric with initial data.
func (m *metricSnowflakeQueuedRepairTimeTotal) init() {
	m.data.SetName("snowflake.queued_repair_time.total")
	m.data.SetDescription("Total time spent in warehouse queue waiting for compute resources to be repaired.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeQueuedRepairTimeTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeQueuedRepairTimeTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeQueuedRepairTimeTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeQueuedRepairTimeTotal(settings MetricSettings) metricSnowflakeQueuedRepairTimeTotal {
	m := metricSnowflakeQueuedRepairTimeTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsDeletedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_deleted.total metric with initial data.
func (m *metricSnowflakeRowsDeletedTotal) init() {
	m.data.SetName("snowflake.rows_deleted.total")
	m.data.SetDescription("Number of rows deleted from a table (or tables).")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsDeletedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsDeletedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsDeletedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsDeletedTotal(settings MetricSettings) metricSnowflakeRowsDeletedTotal {
	m := metricSnowflakeRowsDeletedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsInsertedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_inserted.total metric with initial data.
func (m *metricSnowflakeRowsInsertedTotal) init() {
	m.data.SetName("snowflake.rows_inserted.total")
	m.data.SetDescription("Number of rows inserted into a table (or tables).")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsInsertedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsInsertedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsInsertedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsInsertedTotal(settings MetricSettings) metricSnowflakeRowsInsertedTotal {
	m := metricSnowflakeRowsInsertedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsProducedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_produced.total metric with initial data.
func (m *metricSnowflakeRowsProducedTotal) init() {
	m.data.SetName("snowflake.rows_produced.total")
	m.data.SetDescription("Total number of rows produced by statement.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsProducedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsProducedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsProducedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsProducedTotal(settings MetricSettings) metricSnowflakeRowsProducedTotal {
	m := metricSnowflakeRowsProducedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsUnloadedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_unloaded.total metric with initial data.
func (m *metricSnowflakeRowsUnloadedTotal) init() {
	m.data.SetName("snowflake.rows_unloaded.total")
	m.data.SetDescription("Total number of rows unloaded during data export.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsUnloadedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsUnloadedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsUnloadedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsUnloadedTotal(settings MetricSettings) metricSnowflakeRowsUnloadedTotal {
	m := metricSnowflakeRowsUnloadedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeRowsUpdatedTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.rows_updated.total metric with initial data.
func (m *metricSnowflakeRowsUpdatedTotal) init() {
	m.data.SetName("snowflake.rows_updated.total")
	m.data.SetDescription("Total number of rows updated in a table.")
	m.data.SetUnit("{rows}")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeRowsUpdatedTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeRowsUpdatedTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeRowsUpdatedTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeRowsUpdatedTotal(settings MetricSettings) metricSnowflakeRowsUpdatedTotal {
	m := metricSnowflakeRowsUpdatedTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeSessionIDCount struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.session_id.count metric with initial data.
func (m *metricSnowflakeSessionIDCount) init() {
	m.data.SetName("snowflake.session_id.count")
	m.data.SetDescription("Distinct session id's associated with snowflake username.")
	m.data.SetUnit("1")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeSessionIDCount) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, userNameAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("user_name", userNameAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeSessionIDCount) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeSessionIDCount) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeSessionIDCount(settings MetricSettings) metricSnowflakeSessionIDCount {
	m := metricSnowflakeSessionIDCount{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageFailsafeBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.failsafe_bytes.total metric with initial data.
func (m *metricSnowflakeStorageFailsafeBytesTotal) init() {
	m.data.SetName("snowflake.storage.failsafe_bytes.total")
	m.data.SetDescription("Number of bytes of data in Fail-safe.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageFailsafeBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageFailsafeBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageFailsafeBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageFailsafeBytesTotal(settings MetricSettings) metricSnowflakeStorageFailsafeBytesTotal {
	m := metricSnowflakeStorageFailsafeBytesTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageStageBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.stage_bytes.total metric with initial data.
func (m *metricSnowflakeStorageStageBytesTotal) init() {
	m.data.SetName("snowflake.storage.stage_bytes.total")
	m.data.SetDescription("Number of bytes of stage storage used by files in all internal stages (named, table, user).")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageStageBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageStageBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageStageBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageStageBytesTotal(settings MetricSettings) metricSnowflakeStorageStageBytesTotal {
	m := metricSnowflakeStorageStageBytesTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeStorageStorageBytesTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.storage.storage_bytes.total metric with initial data.
func (m *metricSnowflakeStorageStorageBytesTotal) init() {
	m.data.SetName("snowflake.storage.storage_bytes.total")
	m.data.SetDescription("Number of bytes of table storage used, including bytes for data currently in Time Travel.")
	m.data.SetUnit("By")
	m.data.SetEmptyGauge()
}

func (m *metricSnowflakeStorageStorageBytesTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeStorageStorageBytesTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeStorageStorageBytesTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeStorageStorageBytesTotal(settings MetricSettings) metricSnowflakeStorageStorageBytesTotal {
	m := metricSnowflakeStorageStorageBytesTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeTotalElapsedTimeAvg struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.total_elapsed_time.avg metric with initial data.
func (m *metricSnowflakeTotalElapsedTimeAvg) init() {
	m.data.SetName("snowflake.total_elapsed_time.avg")
	m.data.SetDescription("Average elapsed time.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeTotalElapsedTimeAvg) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetDoubleValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeTotalElapsedTimeAvg) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeTotalElapsedTimeAvg) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeTotalElapsedTimeAvg(settings MetricSettings) metricSnowflakeTotalElapsedTimeAvg {
	m := metricSnowflakeTotalElapsedTimeAvg{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

type metricSnowflakeTotalElapsedTimeTotal struct {
	data     pmetric.Metric // data buffer for generated metric.
	settings MetricSettings // metric settings provided by user.
	capacity int            // max observed number of data points added to the metric.
}

// init fills snowflake.total_elapsed_time.total metric with initial data.
func (m *metricSnowflakeTotalElapsedTimeTotal) init() {
	m.data.SetName("snowflake.total_elapsed_time.total")
	m.data.SetDescription("Total elapsed time.")
	m.data.SetUnit("s")
	m.data.SetEmptyGauge()
	m.data.Gauge().DataPoints().EnsureCapacity(m.capacity)
}

func (m *metricSnowflakeTotalElapsedTimeTotal) recordDataPoint(start pcommon.Timestamp, ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	if !m.settings.Enabled {
		return
	}
	dp := m.data.Gauge().DataPoints().AppendEmpty()
	dp.SetStartTimestamp(start)
	dp.SetTimestamp(ts)
	dp.SetIntValue(val)
	dp.Attributes().PutStr("schema_name", schemaNameAttributeValue)
	dp.Attributes().PutStr("execution_status", executionStatusAttributeValue)
	dp.Attributes().PutStr("error_message", errorMessageAttributeValue)
	dp.Attributes().PutStr("query_type", queryTypeAttributeValue)
	dp.Attributes().PutStr("warehouse_name", warehouseNameAttributeValue)
	dp.Attributes().PutStr("database_name", databaseNameAttributeValue)
	dp.Attributes().PutStr("warehouse_size", warehouseSizeAttributeValue)
}

// updateCapacity saves max length of data point slices that will be used for the slice capacity.
func (m *metricSnowflakeTotalElapsedTimeTotal) updateCapacity() {
	if m.data.Gauge().DataPoints().Len() > m.capacity {
		m.capacity = m.data.Gauge().DataPoints().Len()
	}
}

// emit appends recorded metric data to a metrics slice and prepares it for recording another set of data points.
func (m *metricSnowflakeTotalElapsedTimeTotal) emit(metrics pmetric.MetricSlice) {
	if m.settings.Enabled && m.data.Gauge().DataPoints().Len() > 0 {
		m.updateCapacity()
		m.data.MoveTo(metrics.AppendEmpty())
		m.init()
	}
}

func newMetricSnowflakeTotalElapsedTimeTotal(settings MetricSettings) metricSnowflakeTotalElapsedTimeTotal {
	m := metricSnowflakeTotalElapsedTimeTotal{settings: settings}
	if settings.Enabled {
		m.data = pmetric.NewMetric()
		m.init()
	}
	return m
}

// MetricsBuilder provides an interface for scrapers to report metrics while taking care of all the transformations
// required to produce metric representation defined in metadata and user settings.
type MetricsBuilder struct {
	startTime                                            pcommon.Timestamp   // start time that will be applied to all recorded data points.
	metricsCapacity                                      int                 // maximum observed number of metrics per resource.
	resourceCapacity                                     int                 // maximum observed number of resource attributes.
	metricsBuffer                                        pmetric.Metrics     // accumulates metrics data before emitting.
	buildInfo                                            component.BuildInfo // contains version information
	metricSnowflakeBillingCloudServiceTotal              metricSnowflakeBillingCloudServiceTotal
	metricSnowflakeBillingTotalCreditTotal               metricSnowflakeBillingTotalCreditTotal
	metricSnowflakeBillingVirtualWarehouseTotal          metricSnowflakeBillingVirtualWarehouseTotal
	metricSnowflakeBillingWarehouseCloudServiceTotal     metricSnowflakeBillingWarehouseCloudServiceTotal
	metricSnowflakeBillingWarehouseTotalCreditTotal      metricSnowflakeBillingWarehouseTotalCreditTotal
	metricSnowflakeBillingWarehouseVirtualWarehouseTotal metricSnowflakeBillingWarehouseVirtualWarehouseTotal
	metricSnowflakeDatabaseBytesScannedAvg               metricSnowflakeDatabaseBytesScannedAvg
	metricSnowflakeDatabaseQueryCount                    metricSnowflakeDatabaseQueryCount
	metricSnowflakeLoginsTotal                           metricSnowflakeLoginsTotal
	metricSnowflakePipeCreditsUsedTotal                  metricSnowflakePipeCreditsUsedTotal
	metricSnowflakeQueryBlocked                          metricSnowflakeQueryBlocked
	metricSnowflakeQueryBytesDeletedTotal                metricSnowflakeQueryBytesDeletedTotal
	metricSnowflakeQueryBytesScannedTotal                metricSnowflakeQueryBytesScannedTotal
	metricSnowflakeQueryBytesSpilledLocalTotal           metricSnowflakeQueryBytesSpilledLocalTotal
	metricSnowflakeQueryBytesSpilledRemoteTotal          metricSnowflakeQueryBytesSpilledRemoteTotal
	metricSnowflakeQueryBytesWrittenTotal                metricSnowflakeQueryBytesWrittenTotal
	metricSnowflakeQueryCompilationTimeTotal             metricSnowflakeQueryCompilationTimeTotal
	metricSnowflakeQueryDataScannedCacheAvg              metricSnowflakeQueryDataScannedCacheAvg
	metricSnowflakeQueryExecuted                         metricSnowflakeQueryExecuted
	metricSnowflakeQueryExecutionTimeTotal               metricSnowflakeQueryExecutionTimeTotal
	metricSnowflakeQueryPartitionsScannedTotal           metricSnowflakeQueryPartitionsScannedTotal
	metricSnowflakeQueryQueuedOverload                   metricSnowflakeQueryQueuedOverload
	metricSnowflakeQueryQueuedProvision                  metricSnowflakeQueryQueuedProvision
	metricSnowflakeQueuedOverloadTimeAvg                 metricSnowflakeQueuedOverloadTimeAvg
	metricSnowflakeQueuedOverloadTimeTotal               metricSnowflakeQueuedOverloadTimeTotal
	metricSnowflakeQueuedProvisioningTimeAvg             metricSnowflakeQueuedProvisioningTimeAvg
	metricSnowflakeQueuedProvisioningTimeTotal           metricSnowflakeQueuedProvisioningTimeTotal
	metricSnowflakeQueuedRepairTimeAvg                   metricSnowflakeQueuedRepairTimeAvg
	metricSnowflakeQueuedRepairTimeTotal                 metricSnowflakeQueuedRepairTimeTotal
	metricSnowflakeRowsDeletedTotal                      metricSnowflakeRowsDeletedTotal
	metricSnowflakeRowsInsertedTotal                     metricSnowflakeRowsInsertedTotal
	metricSnowflakeRowsProducedTotal                     metricSnowflakeRowsProducedTotal
	metricSnowflakeRowsUnloadedTotal                     metricSnowflakeRowsUnloadedTotal
	metricSnowflakeRowsUpdatedTotal                      metricSnowflakeRowsUpdatedTotal
	metricSnowflakeSessionIDCount                        metricSnowflakeSessionIDCount
	metricSnowflakeStorageFailsafeBytesTotal             metricSnowflakeStorageFailsafeBytesTotal
	metricSnowflakeStorageStageBytesTotal                metricSnowflakeStorageStageBytesTotal
	metricSnowflakeStorageStorageBytesTotal              metricSnowflakeStorageStorageBytesTotal
	metricSnowflakeTotalElapsedTimeAvg                   metricSnowflakeTotalElapsedTimeAvg
	metricSnowflakeTotalElapsedTimeTotal                 metricSnowflakeTotalElapsedTimeTotal
}

// metricBuilderOption applies changes to default metrics builder.
type metricBuilderOption func(*MetricsBuilder)

// WithStartTime sets startTime on the metrics builder.
func WithStartTime(startTime pcommon.Timestamp) metricBuilderOption {
	return func(mb *MetricsBuilder) {
		mb.startTime = startTime
	}
}

func NewMetricsBuilder(settings MetricsSettings, buildInfo component.BuildInfo, options ...metricBuilderOption) *MetricsBuilder {
	mb := &MetricsBuilder{
		startTime:                                            pcommon.NewTimestampFromTime(time.Now()),
		metricsBuffer:                                        pmetric.NewMetrics(),
		buildInfo:                                            buildInfo,
		metricSnowflakeBillingCloudServiceTotal:              newMetricSnowflakeBillingCloudServiceTotal(settings.SnowflakeBillingCloudServiceTotal),
		metricSnowflakeBillingTotalCreditTotal:               newMetricSnowflakeBillingTotalCreditTotal(settings.SnowflakeBillingTotalCreditTotal),
		metricSnowflakeBillingVirtualWarehouseTotal:          newMetricSnowflakeBillingVirtualWarehouseTotal(settings.SnowflakeBillingVirtualWarehouseTotal),
		metricSnowflakeBillingWarehouseCloudServiceTotal:     newMetricSnowflakeBillingWarehouseCloudServiceTotal(settings.SnowflakeBillingWarehouseCloudServiceTotal),
		metricSnowflakeBillingWarehouseTotalCreditTotal:      newMetricSnowflakeBillingWarehouseTotalCreditTotal(settings.SnowflakeBillingWarehouseTotalCreditTotal),
		metricSnowflakeBillingWarehouseVirtualWarehouseTotal: newMetricSnowflakeBillingWarehouseVirtualWarehouseTotal(settings.SnowflakeBillingWarehouseVirtualWarehouseTotal),
		metricSnowflakeDatabaseBytesScannedAvg:               newMetricSnowflakeDatabaseBytesScannedAvg(settings.SnowflakeDatabaseBytesScannedAvg),
		metricSnowflakeDatabaseQueryCount:                    newMetricSnowflakeDatabaseQueryCount(settings.SnowflakeDatabaseQueryCount),
		metricSnowflakeLoginsTotal:                           newMetricSnowflakeLoginsTotal(settings.SnowflakeLoginsTotal),
		metricSnowflakePipeCreditsUsedTotal:                  newMetricSnowflakePipeCreditsUsedTotal(settings.SnowflakePipeCreditsUsedTotal),
		metricSnowflakeQueryBlocked:                          newMetricSnowflakeQueryBlocked(settings.SnowflakeQueryBlocked),
		metricSnowflakeQueryBytesDeletedTotal:                newMetricSnowflakeQueryBytesDeletedTotal(settings.SnowflakeQueryBytesDeletedTotal),
		metricSnowflakeQueryBytesScannedTotal:                newMetricSnowflakeQueryBytesScannedTotal(settings.SnowflakeQueryBytesScannedTotal),
		metricSnowflakeQueryBytesSpilledLocalTotal:           newMetricSnowflakeQueryBytesSpilledLocalTotal(settings.SnowflakeQueryBytesSpilledLocalTotal),
		metricSnowflakeQueryBytesSpilledRemoteTotal:          newMetricSnowflakeQueryBytesSpilledRemoteTotal(settings.SnowflakeQueryBytesSpilledRemoteTotal),
		metricSnowflakeQueryBytesWrittenTotal:                newMetricSnowflakeQueryBytesWrittenTotal(settings.SnowflakeQueryBytesWrittenTotal),
		metricSnowflakeQueryCompilationTimeTotal:             newMetricSnowflakeQueryCompilationTimeTotal(settings.SnowflakeQueryCompilationTimeTotal),
		metricSnowflakeQueryDataScannedCacheAvg:              newMetricSnowflakeQueryDataScannedCacheAvg(settings.SnowflakeQueryDataScannedCacheAvg),
		metricSnowflakeQueryExecuted:                         newMetricSnowflakeQueryExecuted(settings.SnowflakeQueryExecuted),
		metricSnowflakeQueryExecutionTimeTotal:               newMetricSnowflakeQueryExecutionTimeTotal(settings.SnowflakeQueryExecutionTimeTotal),
		metricSnowflakeQueryPartitionsScannedTotal:           newMetricSnowflakeQueryPartitionsScannedTotal(settings.SnowflakeQueryPartitionsScannedTotal),
		metricSnowflakeQueryQueuedOverload:                   newMetricSnowflakeQueryQueuedOverload(settings.SnowflakeQueryQueuedOverload),
		metricSnowflakeQueryQueuedProvision:                  newMetricSnowflakeQueryQueuedProvision(settings.SnowflakeQueryQueuedProvision),
		metricSnowflakeQueuedOverloadTimeAvg:                 newMetricSnowflakeQueuedOverloadTimeAvg(settings.SnowflakeQueuedOverloadTimeAvg),
		metricSnowflakeQueuedOverloadTimeTotal:               newMetricSnowflakeQueuedOverloadTimeTotal(settings.SnowflakeQueuedOverloadTimeTotal),
		metricSnowflakeQueuedProvisioningTimeAvg:             newMetricSnowflakeQueuedProvisioningTimeAvg(settings.SnowflakeQueuedProvisioningTimeAvg),
		metricSnowflakeQueuedProvisioningTimeTotal:           newMetricSnowflakeQueuedProvisioningTimeTotal(settings.SnowflakeQueuedProvisioningTimeTotal),
		metricSnowflakeQueuedRepairTimeAvg:                   newMetricSnowflakeQueuedRepairTimeAvg(settings.SnowflakeQueuedRepairTimeAvg),
		metricSnowflakeQueuedRepairTimeTotal:                 newMetricSnowflakeQueuedRepairTimeTotal(settings.SnowflakeQueuedRepairTimeTotal),
		metricSnowflakeRowsDeletedTotal:                      newMetricSnowflakeRowsDeletedTotal(settings.SnowflakeRowsDeletedTotal),
		metricSnowflakeRowsInsertedTotal:                     newMetricSnowflakeRowsInsertedTotal(settings.SnowflakeRowsInsertedTotal),
		metricSnowflakeRowsProducedTotal:                     newMetricSnowflakeRowsProducedTotal(settings.SnowflakeRowsProducedTotal),
		metricSnowflakeRowsUnloadedTotal:                     newMetricSnowflakeRowsUnloadedTotal(settings.SnowflakeRowsUnloadedTotal),
		metricSnowflakeRowsUpdatedTotal:                      newMetricSnowflakeRowsUpdatedTotal(settings.SnowflakeRowsUpdatedTotal),
		metricSnowflakeSessionIDCount:                        newMetricSnowflakeSessionIDCount(settings.SnowflakeSessionIDCount),
		metricSnowflakeStorageFailsafeBytesTotal:             newMetricSnowflakeStorageFailsafeBytesTotal(settings.SnowflakeStorageFailsafeBytesTotal),
		metricSnowflakeStorageStageBytesTotal:                newMetricSnowflakeStorageStageBytesTotal(settings.SnowflakeStorageStageBytesTotal),
		metricSnowflakeStorageStorageBytesTotal:              newMetricSnowflakeStorageStorageBytesTotal(settings.SnowflakeStorageStorageBytesTotal),
		metricSnowflakeTotalElapsedTimeAvg:                   newMetricSnowflakeTotalElapsedTimeAvg(settings.SnowflakeTotalElapsedTimeAvg),
		metricSnowflakeTotalElapsedTimeTotal:                 newMetricSnowflakeTotalElapsedTimeTotal(settings.SnowflakeTotalElapsedTimeTotal),
	}
	for _, op := range options {
		op(mb)
	}
	return mb
}

// updateCapacity updates max length of metrics and resource attributes that will be used for the slice capacity.
func (mb *MetricsBuilder) updateCapacity(rm pmetric.ResourceMetrics) {
	if mb.metricsCapacity < rm.ScopeMetrics().At(0).Metrics().Len() {
		mb.metricsCapacity = rm.ScopeMetrics().At(0).Metrics().Len()
	}
	if mb.resourceCapacity < rm.Resource().Attributes().Len() {
		mb.resourceCapacity = rm.Resource().Attributes().Len()
	}
}

// ResourceMetricsOption applies changes to provided resource metrics.
type ResourceMetricsOption func(pmetric.ResourceMetrics)

// WithSnowflakeAccountName sets provided value as "snowflake.account.name" attribute for current resource.
func WithSnowflakeAccountName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("snowflake.account.name", val)
	}
}

// WithSnowflakeUsername sets provided value as "snowflake.username" attribute for current resource.
func WithSnowflakeUsername(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("snowflake.username", val)
	}
}

// WithSnowflakeWarehouseName sets provided value as "snowflake.warehouse.name" attribute for current resource.
func WithSnowflakeWarehouseName(val string) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		rm.Resource().Attributes().PutStr("snowflake.warehouse.name", val)
	}
}

// WithStartTimeOverride overrides start time for all the resource metrics data points.
// This option should be only used if different start time has to be set on metrics coming from different resources.
func WithStartTimeOverride(start pcommon.Timestamp) ResourceMetricsOption {
	return func(rm pmetric.ResourceMetrics) {
		var dps pmetric.NumberDataPointSlice
		metrics := rm.ScopeMetrics().At(0).Metrics()
		for i := 0; i < metrics.Len(); i++ {
			switch metrics.At(i).Type() {
			case pmetric.MetricTypeGauge:
				dps = metrics.At(i).Gauge().DataPoints()
			case pmetric.MetricTypeSum:
				dps = metrics.At(i).Sum().DataPoints()
			}
			for j := 0; j < dps.Len(); j++ {
				dps.At(j).SetStartTimestamp(start)
			}
		}
	}
}

// EmitForResource saves all the generated metrics under a new resource and updates the internal state to be ready for
// recording another set of data points as part of another resource. This function can be helpful when one scraper
// needs to emit metrics from several resources. Otherwise calling this function is not required,
// just `Emit` function can be called instead.
// Resource attributes should be provided as ResourceMetricsOption arguments.
func (mb *MetricsBuilder) EmitForResource(rmo ...ResourceMetricsOption) {
	rm := pmetric.NewResourceMetrics()
	rm.Resource().Attributes().EnsureCapacity(mb.resourceCapacity)
	ils := rm.ScopeMetrics().AppendEmpty()
	ils.Scope().SetName("otelcol/snowflakereceiver")
	ils.Scope().SetVersion(mb.buildInfo.Version)
	ils.Metrics().EnsureCapacity(mb.metricsCapacity)
	mb.metricSnowflakeBillingCloudServiceTotal.emit(ils.Metrics())
	mb.metricSnowflakeBillingTotalCreditTotal.emit(ils.Metrics())
	mb.metricSnowflakeBillingVirtualWarehouseTotal.emit(ils.Metrics())
	mb.metricSnowflakeBillingWarehouseCloudServiceTotal.emit(ils.Metrics())
	mb.metricSnowflakeBillingWarehouseTotalCreditTotal.emit(ils.Metrics())
	mb.metricSnowflakeBillingWarehouseVirtualWarehouseTotal.emit(ils.Metrics())
	mb.metricSnowflakeDatabaseBytesScannedAvg.emit(ils.Metrics())
	mb.metricSnowflakeDatabaseQueryCount.emit(ils.Metrics())
	mb.metricSnowflakeLoginsTotal.emit(ils.Metrics())
	mb.metricSnowflakePipeCreditsUsedTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryBlocked.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesDeletedTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesScannedTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesSpilledLocalTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesSpilledRemoteTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryBytesWrittenTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryCompilationTimeTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryDataScannedCacheAvg.emit(ils.Metrics())
	mb.metricSnowflakeQueryExecuted.emit(ils.Metrics())
	mb.metricSnowflakeQueryExecutionTimeTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryPartitionsScannedTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueryQueuedOverload.emit(ils.Metrics())
	mb.metricSnowflakeQueryQueuedProvision.emit(ils.Metrics())
	mb.metricSnowflakeQueuedOverloadTimeAvg.emit(ils.Metrics())
	mb.metricSnowflakeQueuedOverloadTimeTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueuedProvisioningTimeAvg.emit(ils.Metrics())
	mb.metricSnowflakeQueuedProvisioningTimeTotal.emit(ils.Metrics())
	mb.metricSnowflakeQueuedRepairTimeAvg.emit(ils.Metrics())
	mb.metricSnowflakeQueuedRepairTimeTotal.emit(ils.Metrics())
	mb.metricSnowflakeRowsDeletedTotal.emit(ils.Metrics())
	mb.metricSnowflakeRowsInsertedTotal.emit(ils.Metrics())
	mb.metricSnowflakeRowsProducedTotal.emit(ils.Metrics())
	mb.metricSnowflakeRowsUnloadedTotal.emit(ils.Metrics())
	mb.metricSnowflakeRowsUpdatedTotal.emit(ils.Metrics())
	mb.metricSnowflakeSessionIDCount.emit(ils.Metrics())
	mb.metricSnowflakeStorageFailsafeBytesTotal.emit(ils.Metrics())
	mb.metricSnowflakeStorageStageBytesTotal.emit(ils.Metrics())
	mb.metricSnowflakeStorageStorageBytesTotal.emit(ils.Metrics())
	mb.metricSnowflakeTotalElapsedTimeAvg.emit(ils.Metrics())
	mb.metricSnowflakeTotalElapsedTimeTotal.emit(ils.Metrics())
	for _, op := range rmo {
		op(rm)
	}
	if ils.Metrics().Len() > 0 {
		mb.updateCapacity(rm)
		rm.MoveTo(mb.metricsBuffer.ResourceMetrics().AppendEmpty())
	}
}

// Emit returns all the metrics accumulated by the metrics builder and updates the internal state to be ready for
// recording another set of metrics. This function will be responsible for applying all the transformations required to
// produce metric representation defined in metadata and user settings, e.g. delta or cumulative.
func (mb *MetricsBuilder) Emit(rmo ...ResourceMetricsOption) pmetric.Metrics {
	mb.EmitForResource(rmo...)
	metrics := pmetric.NewMetrics()
	mb.metricsBuffer.MoveTo(metrics)
	return metrics
}

// RecordSnowflakeBillingCloudServiceTotalDataPoint adds a data point to snowflake.billing.cloud_service.total metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingCloudServiceTotalDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	mb.metricSnowflakeBillingCloudServiceTotal.recordDataPoint(mb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingTotalCreditTotalDataPoint adds a data point to snowflake.billing.total_credit.total metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingTotalCreditTotalDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	mb.metricSnowflakeBillingTotalCreditTotal.recordDataPoint(mb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingVirtualWarehouseTotalDataPoint adds a data point to snowflake.billing.virtual_warehouse.total metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingVirtualWarehouseTotalDataPoint(ts pcommon.Timestamp, val float64, serviceTypeAttributeValue string) {
	mb.metricSnowflakeBillingVirtualWarehouseTotal.recordDataPoint(mb.startTime, ts, val, serviceTypeAttributeValue)
}

// RecordSnowflakeBillingWarehouseCloudServiceTotalDataPoint adds a data point to snowflake.billing.warehouse.cloud_service.total metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingWarehouseCloudServiceTotalDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeBillingWarehouseCloudServiceTotal.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeBillingWarehouseTotalCreditTotalDataPoint adds a data point to snowflake.billing.warehouse.total_credit.total metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingWarehouseTotalCreditTotalDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeBillingWarehouseTotalCreditTotal.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeBillingWarehouseVirtualWarehouseTotalDataPoint adds a data point to snowflake.billing.warehouse.virtual_warehouse.total metric.
func (mb *MetricsBuilder) RecordSnowflakeBillingWarehouseVirtualWarehouseTotalDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeBillingWarehouseVirtualWarehouseTotal.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeDatabaseBytesScannedAvgDataPoint adds a data point to snowflake.database.bytes_scanned.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeDatabaseBytesScannedAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeDatabaseBytesScannedAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeDatabaseQueryCountDataPoint adds a data point to snowflake.database.query.count metric.
func (mb *MetricsBuilder) RecordSnowflakeDatabaseQueryCountDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeDatabaseQueryCount.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeLoginsTotalDataPoint adds a data point to snowflake.logins.total metric.
func (mb *MetricsBuilder) RecordSnowflakeLoginsTotalDataPoint(ts pcommon.Timestamp, val int64, errorMessageAttributeValue string, reportedClientTypeAttributeValue string, isSuccessAttributeValue string) {
	mb.metricSnowflakeLoginsTotal.recordDataPoint(mb.startTime, ts, val, errorMessageAttributeValue, reportedClientTypeAttributeValue, isSuccessAttributeValue)
}

// RecordSnowflakePipeCreditsUsedTotalDataPoint adds a data point to snowflake.pipe.credits_used.total metric.
func (mb *MetricsBuilder) RecordSnowflakePipeCreditsUsedTotalDataPoint(ts pcommon.Timestamp, val float64, pipeNameAttributeValue string) {
	mb.metricSnowflakePipeCreditsUsedTotal.recordDataPoint(mb.startTime, ts, val, pipeNameAttributeValue)
}

// RecordSnowflakeQueryBlockedDataPoint adds a data point to snowflake.query.blocked metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBlockedDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeQueryBlocked.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryBytesDeletedTotalDataPoint adds a data point to snowflake.query.bytes_deleted.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesDeletedTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesDeletedTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesScannedTotalDataPoint adds a data point to snowflake.query.bytes_scanned.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesScannedTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesScannedTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesSpilledLocalTotalDataPoint adds a data point to snowflake.query.bytes_spilled.local.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesSpilledLocalTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesSpilledLocalTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesSpilledRemoteTotalDataPoint adds a data point to snowflake.query.bytes_spilled.remote.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesSpilledRemoteTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesSpilledRemoteTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryBytesWrittenTotalDataPoint adds a data point to snowflake.query.bytes_written.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryBytesWrittenTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryBytesWrittenTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryCompilationTimeTotalDataPoint adds a data point to snowflake.query.compilation_time.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryCompilationTimeTotalDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryCompilationTimeTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryDataScannedCacheAvgDataPoint adds a data point to snowflake.query.data_scanned_cache.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryDataScannedCacheAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryDataScannedCacheAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryExecutedDataPoint adds a data point to snowflake.query.executed metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryExecutedDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeQueryExecuted.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryExecutionTimeTotalDataPoint adds a data point to snowflake.query.execution_time.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryExecutionTimeTotalDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryExecutionTimeTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryPartitionsScannedTotalDataPoint adds a data point to snowflake.query.partitions_scanned.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryPartitionsScannedTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueryPartitionsScannedTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueryQueuedOverloadDataPoint adds a data point to snowflake.query.queued_overload metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryQueuedOverloadDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeQueryQueuedOverload.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueryQueuedProvisionDataPoint adds a data point to snowflake.query.queued_provision metric.
func (mb *MetricsBuilder) RecordSnowflakeQueryQueuedProvisionDataPoint(ts pcommon.Timestamp, val float64, warehouseNameAttributeValue string) {
	mb.metricSnowflakeQueryQueuedProvision.recordDataPoint(mb.startTime, ts, val, warehouseNameAttributeValue)
}

// RecordSnowflakeQueuedOverloadTimeAvgDataPoint adds a data point to snowflake.queued_overload_time.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedOverloadTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedOverloadTimeAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedOverloadTimeTotalDataPoint adds a data point to snowflake.queued_overload_time.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedOverloadTimeTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedOverloadTimeTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedProvisioningTimeAvgDataPoint adds a data point to snowflake.queued_provisioning_time.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedProvisioningTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedProvisioningTimeAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedProvisioningTimeTotalDataPoint adds a data point to snowflake.queued_provisioning_time.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedProvisioningTimeTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedProvisioningTimeTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedRepairTimeAvgDataPoint adds a data point to snowflake.queued_repair_time.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedRepairTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedRepairTimeAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeQueuedRepairTimeTotalDataPoint adds a data point to snowflake.queued_repair_time.total metric.
func (mb *MetricsBuilder) RecordSnowflakeQueuedRepairTimeTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeQueuedRepairTimeTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsDeletedTotalDataPoint adds a data point to snowflake.rows_deleted.total metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsDeletedTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsDeletedTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsInsertedTotalDataPoint adds a data point to snowflake.rows_inserted.total metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsInsertedTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsInsertedTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsProducedTotalDataPoint adds a data point to snowflake.rows_produced.total metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsProducedTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsProducedTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsUnloadedTotalDataPoint adds a data point to snowflake.rows_unloaded.total metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsUnloadedTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsUnloadedTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeRowsUpdatedTotalDataPoint adds a data point to snowflake.rows_updated.total metric.
func (mb *MetricsBuilder) RecordSnowflakeRowsUpdatedTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeRowsUpdatedTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeSessionIDCountDataPoint adds a data point to snowflake.session_id.count metric.
func (mb *MetricsBuilder) RecordSnowflakeSessionIDCountDataPoint(ts pcommon.Timestamp, val int64, userNameAttributeValue string) {
	mb.metricSnowflakeSessionIDCount.recordDataPoint(mb.startTime, ts, val, userNameAttributeValue)
}

// RecordSnowflakeStorageFailsafeBytesTotalDataPoint adds a data point to snowflake.storage.failsafe_bytes.total metric.
func (mb *MetricsBuilder) RecordSnowflakeStorageFailsafeBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricSnowflakeStorageFailsafeBytesTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordSnowflakeStorageStageBytesTotalDataPoint adds a data point to snowflake.storage.stage_bytes.total metric.
func (mb *MetricsBuilder) RecordSnowflakeStorageStageBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricSnowflakeStorageStageBytesTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordSnowflakeStorageStorageBytesTotalDataPoint adds a data point to snowflake.storage.storage_bytes.total metric.
func (mb *MetricsBuilder) RecordSnowflakeStorageStorageBytesTotalDataPoint(ts pcommon.Timestamp, val int64) {
	mb.metricSnowflakeStorageStorageBytesTotal.recordDataPoint(mb.startTime, ts, val)
}

// RecordSnowflakeTotalElapsedTimeAvgDataPoint adds a data point to snowflake.total_elapsed_time.avg metric.
func (mb *MetricsBuilder) RecordSnowflakeTotalElapsedTimeAvgDataPoint(ts pcommon.Timestamp, val float64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeTotalElapsedTimeAvg.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// RecordSnowflakeTotalElapsedTimeTotalDataPoint adds a data point to snowflake.total_elapsed_time.total metric.
func (mb *MetricsBuilder) RecordSnowflakeTotalElapsedTimeTotalDataPoint(ts pcommon.Timestamp, val int64, schemaNameAttributeValue string, executionStatusAttributeValue string, errorMessageAttributeValue string, queryTypeAttributeValue string, warehouseNameAttributeValue string, databaseNameAttributeValue string, warehouseSizeAttributeValue string) {
	mb.metricSnowflakeTotalElapsedTimeTotal.recordDataPoint(mb.startTime, ts, val, schemaNameAttributeValue, executionStatusAttributeValue, errorMessageAttributeValue, queryTypeAttributeValue, warehouseNameAttributeValue, databaseNameAttributeValue, warehouseSizeAttributeValue)
}

// Reset resets metrics builder to its initial state. It should be used when external metrics source is restarted,
// and metrics builder should update its startTime and reset it's internal state accordingly.
func (mb *MetricsBuilder) Reset(options ...metricBuilderOption) {
	mb.startTime = pcommon.NewTimestampFromTime(time.Now())
	for _, op := range options {
		op(mb)
	}
}
